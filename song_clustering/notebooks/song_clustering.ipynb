{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cluster Spotify Songs\n",
    "\n",
    "The data consist of 25 parameters from ~300k spotify songs.\n",
    "These parameters include identifying information (e.g., artist, album, track_id) as well as various song-related features (e.g., 'danceability', 'energy', 'key').\n",
    "We're going to explore different methods for clustering these songs.\n",
    "Accurate clustering would allow a recommendation algorithm to suggest new songs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### load modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = '/Users/andrew/Documents/GitHub/data_science/song_clustering/data/'\n",
    "df = pd.read_csv(data_path + 'tracks_features_subsample.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Description of categories\n",
    "(descriptions taken from Spotify's website)\n",
    "Starting with the less-than-obvious columns.\n",
    "\n",
    "danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "\n",
    "energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "\n",
    "acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
    "\n",
    "instrumentalness: Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "\n",
    "liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n",
    "\n",
    "loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n",
    "\n",
    "mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
    "\n",
    "speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
    "\n",
    "valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "\n",
    "key: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n",
    "\n",
    "time_signature: An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of \"3/4\", to \"7/4\".\n",
    "\n",
    "tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group tracks into genres"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# start by pulling out all of the relevant columns\n",
    "X_all = df[['explicit','danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','valence','tempo','duration_ms','time_signature','year']]\n",
    "\n",
    "# further downselect to only the non-categorical features\n",
    "X_cont = X_all[['danceability','energy','loudness','speechiness','acousticness','instrumentalness','valence','tempo','duration_ms','year']]\n",
    "\n",
    "# normalize loudness, tempo, duration_ms, and year to be between 0-1 (like the other variables)\n",
    "temp_list = ['tempo','duration_ms','year','loudness']\n",
    "for ii in temp_list:\n",
    "    d = X_cont[ii]\n",
    "    X_cont.loc[:,[ii]] = (d - np.min(d)) / (np.max(d) - np.min(d))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run PCA on the continuous variables\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# convert to array\n",
    "X_cont_array = np.array(X_cont)\n",
    "\n",
    "# fit model\n",
    "pca = PCA(n_components = None)\n",
    "proj = pca.fit_transform(X_cont_array);\n",
    "\n",
    "# pull out weights\n",
    "W = pca.components_\n",
    "\n",
    "# pull out the explained variance (fraction of total variance)\n",
    "varExp = pca.explained_variance_ratio_\n",
    "\n",
    "# plot variance explained\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121);\n",
    "plt.plot(varExp);\n",
    "plt.title('fractional variance explained by PCs');\n",
    "\n",
    "# plot the loadings for the PCs\n",
    "plt.subplot(122);\n",
    "plt.imshow(W, cmap='PuOr');\n",
    "plt.colorbar();\n",
    "plt.ylabel('PCs');\n",
    "plt.clim([-0.75, 0.75]);\n",
    "plt.xticks(np.arange(proj.shape[1]), labels = X_cont.columns, rotation=90);\n",
    "plt.title('loadings for PCs');\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### duration and year aren't doing us any good (PCs 9 and 10 are just duration and year), so toss these and re-run PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop duration and year\n",
    "X_cont.drop(['duration_ms','year'],axis=1, inplace=True)\n",
    "\n",
    "# convert to array\n",
    "X_cont_array = np.array(X_cont)\n",
    "\n",
    "# fit model\n",
    "pca = PCA(n_components = None)\n",
    "proj = pca.fit_transform(X_cont_array);\n",
    "\n",
    "# pull out weights\n",
    "W = pca.components_\n",
    "\n",
    "# pull out the explained variance (fraction of total variance)\n",
    "varExp = pca.explained_variance_ratio_\n",
    "\n",
    "# plot variance explained\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121);\n",
    "plt.plot(varExp);\n",
    "plt.title('fractional variance explained by PCs');\n",
    "\n",
    "# plot the loadings for the PCs\n",
    "plt.subplot(122);\n",
    "plt.imshow(W, cmap='PuOr');\n",
    "plt.colorbar();\n",
    "plt.ylabel('PCs');\n",
    "plt.clim([-0.75, 0.75]);\n",
    "plt.xticks(np.arange(proj.shape[1]), labels = X_cont.columns, rotation=90);\n",
    "plt.title('loadings for PCs');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the songs in a few different spaces\n",
    "plt.figure(figsize=(12,3));\n",
    "spaces_to_plot = np.arange(8).reshape(-1,2,order='C');\n",
    "for ii in range(4):\n",
    "    ax = plt.subplot(1,4,ii+1);\n",
    "    plt.plot(proj[:,spaces_to_plot[ii,0]],proj[:,spaces_to_plot[ii,1]], '.', ms=0.01);\n",
    "    plt.xlabel('PC ' + str(spaces_to_plot[ii,0]))\n",
    "    plt.ylabel('PC ' + str(spaces_to_plot[ii,1]))\n",
    "    ax.set_xticks([]);\n",
    "    ax.set_yticks([]);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### do a bit of preprocessing\n",
    "convert categorical variables to dummy variables.\n",
    "not normalizing or centering because non-categorical variables are already on about the same scale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert categorical (or binary) variables to dummy variables (one-hot encoded)\n",
    "catVars = ['Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC', 'MTRANS']\n",
    "\n",
    "for ii in catVars:\n",
    "\n",
    "    # create the dummy variable\n",
    "    dummy = pd.get_dummies(df[ii], drop_first=True)\n",
    "\n",
    "    # remove the original variable\n",
    "    df.drop(ii, axis = 1, inplace=True)\n",
    "\n",
    "    # add the dummified variable\n",
    "    df = pd.concat([df, dummy],axis=1)\n",
    "\n",
    "# drop height and weight from table\n",
    "df.drop(['Height','Weight'], axis=1, inplace=True)\n",
    "\n",
    "# convert the variable referencing 'consumption of food between meals' to integer values\n",
    "vals = np.arange(4)\n",
    "keywords = ['no', 'Sometimes', 'Frequently', 'Always']\n",
    "for ii, kw in enumerate(keywords):\n",
    "    df.loc[df['CAEC']==kw, 'CAEC'] = vals[ii]\n",
    "\n",
    "# do the same with alcohol consumption\n",
    "vals = np.arange(3)\n",
    "keywords = ['no','Sometimes','Frequently']\n",
    "for ii, kw in enumerate(keywords):\n",
    "    df.loc[df['CALC']==kw, 'CALC'] = vals[ii]\n",
    "\n",
    "# and the obesity level\n",
    "vals = np.arange(7)\n",
    "keywords = ['Insufficient_Weight','Normal_Weight','Overweight_Level_I','Overweight_Level_II',\n",
    "            'Obesity_Type_I','Obesity_Type_II','Obesity_Type_III']\n",
    "for ii, kw in enumerate(keywords):\n",
    "    df.loc[df['NObeyesdad']==kw, 'NObeyesdad'] = vals[ii]\n",
    "\n",
    "# convert the obesity level to a binary variable\n",
    "df.loc[df['NObeyesdad'] < 4, 'NObeyesdad'] = 0\n",
    "df.loc[df['NObeyesdad'] >= 4,'NObeyesdad'] = 1\n",
    "\n",
    "# toss the id column\n",
    "df.drop('id', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# separate out our training and test data\n",
    "# fraction of data to use for training\n",
    "train_fraction = 0.9\n",
    "num_train = round(df.shape[0] * train_fraction)\n",
    "\n",
    "df_train = df.iloc[:num_train]\n",
    "df_test  = df.iloc[num_train:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression\n",
    "This will be our baseline\n",
    "no regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# pull out the train X and train Y data\n",
    "X_train = np.array(df_train.iloc[:, df.columns != 'NObeyesdad'])\n",
    "Y_train = np.array(df_train.iloc[:, df.columns == 'NObeyesdad']).astype('bool')[:,0]\n",
    "\n",
    "# pull out the test data\n",
    "X_test = np.array(df_test.iloc[:, df.columns != 'NObeyesdad'])\n",
    "Y_test = np.array(df_test.iloc[:, df.columns == 'NObeyesdad']).astype('bool')[:,0]\n",
    "\n",
    "# initialize our model\n",
    "model_logReg = LogisticRegression(penalty=None, max_iter=500)\n",
    "\n",
    "# fit the model\n",
    "model_logReg.fit(X_train,Y_train)\n",
    "\n",
    "# test the model\n",
    "Y_hat = model_logReg.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "baseline_score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(baseline_score))\n",
    "\n",
    "# initialize a dictionary to keep track of our scores\n",
    "all_scores = {'baseline': baseline_score}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Not bad (certainly above chance), but we can clearly do better\n",
    "as a simple start, at L2 regularization to combat overfitting (likely not an issue, given the small number of parameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# magnitude of L2 penalty (0-1, with lower values corresponding to a greater penalty)\n",
    "l2_lam = np.logspace(-5,stop=1, num=8)[::-1]\n",
    "num_lams = len(l2_lam)\n",
    "\n",
    "# fit each in turn\n",
    "model_logReg_l2 = [None] * num_lams\n",
    "\n",
    "# list to keep track of scores with different l2 penalties\n",
    "l2_score = []\n",
    "for ii,ll in enumerate(l2_lam):\n",
    "\n",
    "    model_logReg_l2[ii] = LogisticRegression(penalty='l2', C=ll, max_iter=500 )\n",
    "\n",
    "    # fit the model\n",
    "    model_logReg_l2[ii].fit(X_train,Y_train)\n",
    "\n",
    "    # test the model\n",
    "    Y_hat = model_logReg_l2[ii].predict(X_test)\n",
    "\n",
    "    # calculate accuracy\n",
    "    score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "    l2_score.append(score)\n",
    "\n",
    "    # display result as fractional improvement over baseline\n",
    "    pct_dScore = (score - baseline_score) / baseline_score\n",
    "    print('percent change in classification accuracy: ' + str(pct_dScore))\n",
    "\n",
    "# add the best score to the dictionary\n",
    "all_scores['l2_reg'] = np.max(l2_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Very tiny improvement in performance with moderate normalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Before we change up the model architecture, let's see if we can get any improvements by handling the input data more intelligently (i.e., 'feature engineering') .\n",
    "First, we're going to bin the ages by decade (whether a person is 30 or 33 likely does not greatly change their likelihood of being obese)\n",
    "We're also going to log-transform the ages, as the fraction of obese individuals changes dramatically as a function of age during early-middle age\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add a 'binned age' column to the dataframe\n",
    "df['age_bin'] = pd.cut(df['Age'], bins=[10,20,30,40,50,60,70],labels=[0,1,2,3,4,5])\n",
    "\n",
    "# log transform the ages\n",
    "df['log_age'] = np.log1p(df['Age'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pull out the test and train data again\n",
    "X_df = df.copy()\n",
    "Y = np.array(X_df['NObeyesdad']).astype('bool')\n",
    "X_df.drop(['NObeyesdad'],axis=1,inplace=True)\n",
    "X = np.array(X_df)\n",
    "\n",
    "\n",
    "# pull out the train X and train Y data\n",
    "X_train = X[:num_train,:]\n",
    "Y_train = Y[:num_train]\n",
    "\n",
    "# pull out the test data\n",
    "X_test = X[num_train:,:]\n",
    "Y_test = Y[num_train:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train models (with l2 regularization and feature engineering)\n",
    "model_logReg_l2_fe = [None] * num_lams\n",
    "\n",
    "# list to keep track of scores\n",
    "fe_scores = []\n",
    "for ii,ll in enumerate(l2_lam):\n",
    "\n",
    "    model_logReg_l2_fe[ii] = LogisticRegression(penalty='l2', C=ll, max_iter = 500 )\n",
    "\n",
    "    # fit the model\n",
    "    model_logReg_l2_fe[ii].fit(X_train,Y_train)\n",
    "\n",
    "    # test the model\n",
    "    Y_hat = model_logReg_l2_fe[ii].predict(X_test)\n",
    "\n",
    "    # calculate accuracy\n",
    "    score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "    fe_scores.append(score)\n",
    "\n",
    "    # display result as fractional improvement over baseline\n",
    "    pct_dScore = (score - baseline_score) / baseline_score\n",
    "    print('percent change in classification accuracy: ' + str(pct_dScore))\n",
    "\n",
    "# save the best score\n",
    "all_scores['feature_engineering'] = np.max(fe_scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### So that feature engineering (plus regularization) brought us from ~0.8 to ~0.82 accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now it's time to switch things up with a different model architecture\n",
    "We're going to start with a feedforward network (relu units, trained with dropout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define some parameters for our network\n",
    "num_units_per_layer = 300\n",
    "num_hidden_layers = 10\n",
    "batch_size = 1000\n",
    "num_epochs = 500\n",
    "dropout_p  = 0.1\n",
    "input_dim = X_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/andrew/Documents/GitHub/data_science/obesity_prediction/code/')\n",
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "import torch\n",
    "\n",
    "# define the dataset and dataloader (both make passing data to the model easier)\n",
    "dataset    = utils.FF_dataset(data=X_train.astype('float16'), targets=Y_train.astype('float16').reshape(-1,1))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "net = utils.FF_network(input_dim=input_dim, num_hidden_layers=num_hidden_layers,\n",
    "                               num_units_per_layer=num_units_per_layer, dropout_p=dropout_p)\n",
    "# define an optimizer (using default ADAM parameters)\n",
    "# get a list of the named parameters\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "# use a simple MSE loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# train the network\n",
    "allLoss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for ii, (x, y) in enumerate(dataloader):\n",
    "\n",
    "        # make sure we're in training mode (the drop-out layers are active)\n",
    "        net.train()\n",
    "\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        out = net(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(out, y)\n",
    "        allLoss.append(loss.detach().numpy())\n",
    "\n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "# plot the loss across batches\n",
    "plt.plot(allLoss);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test on the held-out data\n",
    "net.eval()\n",
    "Y_hat = net(torch.Tensor(X_test.astype('float16')))\n",
    "Y_hat = Y_hat.detach().numpy()[:,0]\n",
    "\n",
    "# convert to prediction\n",
    "Y_hat[Y_hat<0.5] = 0\n",
    "Y_hat[Y_hat>=0.5] = 1\n",
    "\n",
    "# calculate accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "\n",
    "# display result as fractional improvement over baseline\n",
    "pct_dScore = (score - baseline_score) / baseline_score\n",
    "print('classification accuracy: ' + str(score))\n",
    "print('percent change in classification accuracy: ' + str(pct_dScore))\n",
    "\n",
    "# save the score\n",
    "all_scores['ff_net'] = score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now things are looking better.\n",
    "Let's optimize the hyperparameters with a simple grid search\n",
    "Will choose hyperparameters using performance on a validation set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define our hyperparameter grid\n",
    "units_per_layer_grid = np.linspace(200,500,3)\n",
    "num_layers_grid      = np.linspace(5,50,3)\n",
    "p_dropout_grid       = np.linspace(0.05,0.6,3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define our train and validation sets\n",
    "percent_validation = 0.1\n",
    "numValidation = round(X_train.shape[0] * percent_validation)\n",
    "\n",
    "X_val = X_train[:numValidation,:]\n",
    "Y_val = Y_train[:numValidation]\n",
    "X_train_search = X_train[numValidation:,:]\n",
    "Y_train_search = Y_train[numValidation:]\n",
    "\n",
    "# each model is going to be trained until the validation error starts to increase.\n",
    "# define the number of epochs in each 'cycle'\n",
    "num_epochs_per_cycle = 15\n",
    "\n",
    "# define a new dataset and dataloader\n",
    "dataset_search    = utils.FF_dataset(data=X_train_search.astype('float16'), targets=Y_train_search.astype('float16').reshape(-1,1))\n",
    "dataloader_search = DataLoader(dataset_search, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize array to hold validation set performance\n",
    "val_accuracy = np.zeros((3,3,3)) * np.nan\n",
    "\n",
    "# cycle through the hyperparameters\n",
    "for ii,units in enumerate(units_per_layer_grid):\n",
    "    for jj, layers in enumerate(num_layers_grid):\n",
    "        for kk, pDrop in enumerate(p_dropout_grid):\n",
    "\n",
    "            # initialize the network\n",
    "            net = utils.FF_network(input_dim=input_dim, num_hidden_layers=int(layers),\n",
    "                                           num_units_per_layer=int(units), dropout_p=pDrop)\n",
    "\n",
    "            # define an optimizer (using default ADAM parameters)\n",
    "            # get a list of the named parameters\n",
    "            optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "            # use a simple MSE loss function\n",
    "            criterion = torch.nn.BCELoss()\n",
    "\n",
    "            # keep track of the validation loss\n",
    "            valScore = -np.inf\n",
    "            contFlg = True\n",
    "            cycleCounter=0\n",
    "\n",
    "            # run through epochs as long as the validation loss is decreasing\n",
    "            while contFlg:\n",
    "                display('cycle number: ' + str(cycleCounter))\n",
    "\n",
    "                for epoch in range(num_epochs_per_cycle):\n",
    "\n",
    "                    for i, (x, y) in enumerate(dataloader_search):\n",
    "\n",
    "                        # make sure we're in training mode (the drop-out layers are active)\n",
    "                        net.train()\n",
    "\n",
    "                        # clear the gradient\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # forward pass\n",
    "                        out = net(x)\n",
    "\n",
    "                        # calculate loss\n",
    "                        loss = criterion(out, y)\n",
    "\n",
    "                        # calculate gradient\n",
    "                        loss.backward()\n",
    "\n",
    "                        # update parameters\n",
    "                        optimizer.step()\n",
    "\n",
    "                # calculate loss on validation data\n",
    "                net.eval()\n",
    "                Y_hat = net(torch.Tensor(X_val.astype('float16')))\n",
    "                Y_hat = Y_hat.detach().numpy()[:,0]\n",
    "\n",
    "                # convert to prediction\n",
    "                Y_hat[Y_hat<0.5] = 0\n",
    "                Y_hat[Y_hat>=0.5] = 1\n",
    "\n",
    "                # calculate accuracy\n",
    "                score = sum(Y_val == Y_hat) / len(Y_test)\n",
    "\n",
    "                # check to see if validation loss decreased\n",
    "                if score > valScore:\n",
    "                    valScore = np.copy(score)\n",
    "                    cycleCounter+=1\n",
    "                else:\n",
    "                    contFlg = False\n",
    "                    val_accuracy[ii,jj,kk] = valScore\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# interpolate to get higher resolution\n",
    "units_per_layer_grid_int = np.linspace(200,500,10)\n",
    "num_layers_grid_int     = np.linspace(5,50,10)\n",
    "p_dropout_grid_int      = np.linspace(0.05,0.6,10)\n",
    "\n",
    "# generate 2D array of our known data points\n",
    "xv, yv, zv = np.meshgrid(units_per_layer_grid, num_layers_grid, p_dropout_grid, indexing='ij')\n",
    "points = np.concatenate([x.reshape(-1,1,order='F') for x in [xv, yv, zv]],axis=1)\n",
    "\n",
    "# make a 2D array of the desired data point\n",
    "xv, yv, zv = np.meshgrid(units_per_layer_grid_int, num_layers_grid_int, p_dropout_grid_int, indexing='ij')\n",
    "\n",
    "# flatten our values\n",
    "val_flat = val_accuracy.reshape(-1,1,order='F');\n",
    "\n",
    "# interpolate\n",
    "interp_grid = scipy.interpolate.griddata(points, val_flat, (xv, yv, zv), method='linear')\n",
    "\n",
    "# find the maximum (interpolated) value)\n",
    "m_ii, m_jj, m_kk = np.unravel_index(np.argmax(interp_grid), (10,10,10), order='C')\n",
    "\n",
    "# pull out the parameters associated with the maximum value\n",
    "units_per_layer_opt = units_per_layer_grid_int[m_ii]\n",
    "num_layers_grid_opt = num_layers_grid_int[m_jj]\n",
    "p_dropout_grid_opt  = p_dropout_grid_int[m_kk]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train a new network with these parameters\n",
    "\n",
    "# up the number of epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "# initialize the network\n",
    "net = utils.FF_network(input_dim=input_dim, num_hidden_layers=int(num_layers_grid_opt),\n",
    "                               num_units_per_layer=int(units_per_layer_opt), dropout_p=p_dropout_grid_opt)\n",
    "\n",
    "# define an optimizer (using default ADAM parameters)\n",
    "# get a list of the named parameters\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "# use a simple MSE loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# train the network\n",
    "allLoss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for ii, (x, y) in enumerate(dataloader):\n",
    "\n",
    "        # make sure we're in training mode (the drop-out layers are active)\n",
    "        net.train()\n",
    "\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        out = net(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(out, y)\n",
    "        allLoss.append(loss.detach().numpy())\n",
    "\n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "# plot the loss\n",
    "plt.plot(allLoss);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test on the held-out data\n",
    "net.eval()\n",
    "Y_hat = net(torch.Tensor(X_test.astype('float16')))\n",
    "Y_hat = Y_hat.detach().numpy()[:,0]\n",
    "\n",
    "# convert to prediction\n",
    "Y_hat[Y_hat<0.5] = 0\n",
    "Y_hat[Y_hat>=0.5] = 1\n",
    "\n",
    "# calculate accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "\n",
    "# save accuracy\n",
    "all_scores['ff_net_opt'] = score\n",
    "\n",
    "# display result as fractional improvement over baseline\n",
    "pct_dScore = (score - baseline_score) / baseline_score\n",
    "print('classification accuracy: ' + str(score))\n",
    "print('percent change in classification accuracy: ' + str(pct_dScore))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Things are looking much better now, but let's see how much things improve by 'ensembling' models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### our first ensemble is going to be a random forest classifier.\n",
    "Before we get to that, let's look at how a single tree performs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))\n",
    "\n",
    "# save score\n",
    "all_scores['decision_tree'] = score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### surprisingly, a simple decision tree nearly beats our optimized network!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### train a random forest classifier\n",
    "very simple emsemble method. We're going to train a bunch of individual decision trees on bootstrapped collections of the data. This method nicely combats overfitting.\n",
    "the final prediction is made by allowing each tree to 'vote'. Most popular classification wins.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "# number of trees in our forest\n",
    "num_trees = 100\n",
    "\n",
    "# for each tree, bootstrap our training data\n",
    "\n",
    "# number of training samples\n",
    "num_samples  = X_train.shape[0]\n",
    "\n",
    "# list to hold our trees\n",
    "Trees = []\n",
    "for ii in range(num_trees):\n",
    "\n",
    "    # get the bootstrapped data\n",
    "    idx = choice(np.arange(num_samples), num_samples)\n",
    "    Trees.append(tree.DecisionTreeClassifier().fit(X_train[idx,:], Y_train[idx]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the predictions of each tree within the forest (on the test data). Count votes to decide final answer\n",
    "test_out = [tree.predict(X_test) for tree in Trees]\n",
    "\n",
    "# convert into array\n",
    "test_out_array = np.array(test_out)\n",
    "\n",
    "# find most popular output for each input\n",
    "Y_hat = np.sum(test_out_array, axis=0) >= (num_trees/2)\n",
    "\n",
    "# calculate accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))\n",
    "\n",
    "# save result\n",
    "all_scores['random_forest_scratch'] = score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### the random forest classifier is beating our network handily!\n",
    "just for comparison, use the built-in random forest classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "Y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))\n",
    "\n",
    "# save result\n",
    "all_scores['random_forest_sklearn'] = score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now we're easily beating our network\n",
    "Seems like we top out at ~0.907 classification accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Try a gradient boosting classifier\n",
    "Another popular ensemble method. Here, we add decision trees linearly, and train each successive tree on the gradient (loss wrt change in tree parameters) of the previous tree."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(validation_fraction = 0.1, n_estimators = 1000)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "Y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))\n",
    "\n",
    "# save result\n",
    "all_scores['gradiant_boosting'] = score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The kitchen sink\n",
    "Here, we're going to train a few different ensemble methods, and use the output of each ensemble as input to a final logistic classifier.\n",
    "While this method could very likely outperform all others (for obvious reasons), doing so may require a significant amount of hyperparameter optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import all of the classifiers we haven't already\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# number of estimators per base ensemble\n",
    "n_estimators = 500\n",
    "\n",
    "# make a list of the estimators\n",
    "estimators = [('ada', AdaBoostClassifier(n_estimators = n_estimators)),\n",
    "              ('bag', BaggingClassifier(n_estimators = n_estimators)),\n",
    "              ('ext', ExtraTreesClassifier(n_estimators = n_estimators)),\n",
    "              ('grd', GradientBoostingClassifier(n_estimators = n_estimators)),\n",
    "              ('rdm', RandomForestClassifier(n_estimators = n_estimators))]\n",
    "\n",
    "clf = StackingClassifier(estimators = estimators, final_estimator=LogisticRegression(), n_jobs=-1)\n",
    "clf.fit(X_train,Y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check accuracy\n",
    "Y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))\n",
    "\n",
    "# save score\n",
    "all_scores['kitchen_sink_emsemble'] = score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summarize performance of all of our classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10));\n",
    "\n",
    "# number of classifiers\n",
    "num_class = len(all_scores)\n",
    "\n",
    "# performance of each classifier\n",
    "score_vals = list(all_scores.values())\n",
    "\n",
    "# name of each classifier\n",
    "class_names = all_scores.keys()\n",
    "\n",
    "# plot\n",
    "for ii in range(num_class):\n",
    "\n",
    "    plt.bar(ii,score_vals[ii]);\n",
    "\n",
    "# clean up\n",
    "plt.xlim([-1,num_class]);\n",
    "plt.ylim([0.5, 1]);\n",
    "plt.ylabel('R$^2$');\n",
    "plt.xlabel('classifier');\n",
    "plt.xticks(ticks=np.arange(num_class), labels=class_names);\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary of training various classifiers\n",
    "\n",
    "decision tree (and ensembles of decision trees) perform comparably (or better) than feedforward networks, while requiring a fraction of the training time.\n",
    "\n",
    "I only got a very modest improvement with my hyperparameter grid search (for the FF network). I likely would see a larger improvement with a more comprehensive/more intelligent search. For example, rather than a simple grid search, I could've used bayesian optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explore the dataset\n",
    "now that we've maxed out our classifier performance, let's use a few of the simpler classifiers to explore the relationship between the various variables and obesity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pull out the names of each of the variables\n",
    "# we appended the dummy variables in the order: 'Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC', 'MTRANS'\n",
    "\n",
    "# pull out the variable names, drop 'NObeyesdad', replace the dummy variable names with more informative names\n",
    "var_names = list(df.columns[df.columns != 'NObeyesdad'])\n",
    "\n",
    "# find the index where we started appending dummy variables\n",
    "gIdx = [ii for ii, x in enumerate(var_names) if x=='Male' or x=='Female'][0]\n",
    "\n",
    "# replace with more informative names\n",
    "var_names[gIdx] = 'Gender'\n",
    "var_names[gIdx+1] = 'family_history_overweight'\n",
    "var_names[gIdx+2] = 'FAVC'\n",
    "var_names[gIdx+3] = 'smoke'\n",
    "var_names[gIdx+4] = 'SCC'\n",
    "transport = ['bike','motorbike','public_transit','walking']\n",
    "for ii in range(5,9):\n",
    "    var_names[gIdx+ii] = transport[ii-5]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train a logistic regression classifier\n",
    "model = LogisticRegression(penalty='l2', C=1e-2, max_iter=500 )\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# test the model\n",
    "Y_hat = model.predict(X_test)\n",
    "\n",
    "# calculate and display accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))\n",
    "\n",
    "# pull out the coefficients\n",
    "coefs = model.coef_[0,:]\n",
    "\n",
    "# plot the coefficients\n",
    "plt.figure(figsize=(30,5));\n",
    "plt.plot(abs(coefs));\n",
    "plt.xticks(np.arange(len(coefs)), labels=var_names);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we didn't normalize our variables, so we can only get a rough idea about how 'important' each of the variables are to the prediction. But from a first pass, it seems that family history of being overweight, consuming food between meals, and whether or not the person takes public transportation to work are the most informative variables\n",
    "\n",
    "To verify this impression, get p-values for each of the coefficients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result.pvalues"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model=sm.Logit(Y_train.astype('float32'),X_train.astype('float32'))\n",
    "result=model.fit()\n",
    "\n",
    "# plot the inverse of the P-values\n",
    "# sort the p-values\n",
    "idx = np.argsort(result.pvalues)\n",
    "\n",
    "# print the variables in order\n",
    "sorted_vars = [var_names[x] for x in idx]\n",
    "print('sorted variables: ')\n",
    "print(sorted_vars);\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is why we need to be careful about interpreting the magnitude of the coefficients if we don't normalize them! 'Log-age' is by far the 'most important' variable (p < 1e-263, while the next closest variable has a value of < 1e-200)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# I'm now going to re-train the model with only the 5 most 'useful' variables\n",
    "X_train_ds = X_train[:,idx[:5]]\n",
    "\n",
    "# train a logistic regression classifier\n",
    "model = LogisticRegression(penalty='l2', C=1e-2, max_iter=500 )\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_ds,Y_train)\n",
    "\n",
    "# test the model\n",
    "Y_hat = model.predict(X_test[:,idx[:5]])\n",
    "\n",
    "# calculate and display accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "after getting rid of 14 of the 19 variables, we're still able to predict nearly as well as if we used all of the variables (98% as well).\n",
    "In fact, if we only use 'log age', we can predict 89% as well (0.72 prediction accuracy on held-out data)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# I'm now going to re-train the model with only the 5 most 'useful' variables\n",
    "X_train_ds = X_train[:,idx[:1]]\n",
    "\n",
    "# train a logistic regression classifier\n",
    "model = LogisticRegression(penalty='l2', C=1e-2, max_iter=500 )\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_ds,Y_train)\n",
    "\n",
    "# test the model\n",
    "Y_hat = model.predict(X_test[:,idx[:1]])\n",
    "\n",
    "# calculate and display accuracy\n",
    "score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "print('classification accuracy: ' + str(score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### To verify that this is true, do a simple analysis where we define single threshold. If an individual's log age is greater than that threshold, they're predicted to be obese.\n",
    "Sweep this threshold across a range and ask how our performance changes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define our thresholds\n",
    "threshold = np.linspace(2.8,3.8,num=20);\n",
    "\n",
    "# pull out the\n",
    "log_age = X_test[:,idx[0]]\n",
    "\n",
    "# keep track of our scores\n",
    "scores = []\n",
    "\n",
    "# run analysis\n",
    "for tt in threshold:\n",
    "    Y_hat = np.zeros_like(Y_test)\n",
    "    Y_hat[log_age >= tt] = 1\n",
    "\n",
    "    # calculate score\n",
    "    score = sum(Y_test == Y_hat) / len(Y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "# plot results\n",
    "plt.plot(threshold,scores);\n",
    "plt.xlabel('log age threshold');\n",
    "plt.ylabel('R$^2$');\n",
    "plt.title('predicting obesity using only log age');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
